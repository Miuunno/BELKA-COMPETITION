{"metadata":{"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":67356,"databundleVersionId":8006601,"sourceType":"competition"}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install rdkit\n!pip install duckdb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install cloud-tpu-client\n!pip install torch_xla","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-21T14:21:59.195842Z","iopub.execute_input":"2024-06-21T14:21:59.196311Z","iopub.status.idle":"2024-06-21T14:22:52.091187Z","shell.execute_reply.started":"2024-06-21T14:21:59.196267Z","shell.execute_reply":"2024-06-21T14:22:52.089485Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: cloud-tpu-client in /opt/conda/lib/python3.10/site-packages (0.10)\nCollecting google-api-python-client==1.8.0 (from cloud-tpu-client)\n  Downloading google_api_python_client-1.8.0-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: oauth2client in /opt/conda/lib/python3.10/site-packages (from cloud-tpu-client) (4.1.3)\nRequirement already satisfied: httplib2<1dev,>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.21.0)\nRequirement already satisfied: google-auth>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (2.26.1)\nRequirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.2.0)\nCollecting google-api-core<2dev,>=1.13.0 (from google-api-python-client==1.8.0->cloud-tpu-client)\n  Downloading google_api_core-1.34.1-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: six<2dev,>=1.6.1 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.16.0)\nRequirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (3.0.1)\nRequirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client) (0.5.1)\nRequirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client) (0.3.0)\nRequirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client) (4.9)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.62.0)\nRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.20.3)\nRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.32.3)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (4.2.4)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.8.0->cloud-tpu-client) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2024.2.2)\nDownloading google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading google_api_core-1.34.1-py3-none-any.whl (120 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.4/120.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0mCollecting torch_xla\n  Downloading torch_xla-2.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (7.9 kB)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from torch_xla) (1.4.0)\nRequirement already satisfied: cloud-tpu-client>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from torch_xla) (0.10)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from torch_xla) (6.0.1)\nCollecting google-api-python-client==1.8.0 (from cloud-tpu-client>=0.10.0->torch_xla)\n  Using cached google_api_python_client-1.8.0-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: oauth2client in /opt/conda/lib/python3.10/site-packages (from cloud-tpu-client>=0.10.0->torch_xla) (4.1.3)\nRequirement already satisfied: httplib2<1dev,>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla) (0.21.0)\nRequirement already satisfied: google-auth>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla) (2.26.1)\nRequirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla) (0.2.0)\nCollecting google-api-core<2dev,>=1.13.0 (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla)\n  Using cached google_api_core-1.34.1-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: six<2dev,>=1.6.1 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla) (1.16.0)\nRequirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla) (3.0.1)\nRequirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch_xla) (0.5.1)\nRequirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch_xla) (0.3.0)\nRequirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch_xla) (4.9)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla) (1.62.0)\nRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla) (3.20.3)\nRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla) (2.32.3)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla) (4.2.4)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla) (2024.2.2)\nDownloading torch_xla-2.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (84.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.3/84.3 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hUsing cached google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\nUsing cached google_api_core-1.34.1-py3-none-any.whl (120 kB)\nInstalling collected packages: google-api-core, google-api-python-client, torch_xla\n  Attempting uninstall: google-api-core\n    Found existing installation: google-api-core 2.11.1\n    Uninstalling google-api-core-2.11.1:\n      Successfully uninstalled google-api-core-2.11.1\n  Attempting uninstall: google-api-python-client\n    Found existing installation: google-api-python-client 2.131.0\n    Uninstalling google-api-python-client-2.131.0:\n      Successfully uninstalled google-api-python-client-2.131.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nearthengine-api 0.1.405 requires google-api-python-client>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed google-api-core-1.34.1 google-api-python-client-1.8.0 torch_xla-2.3.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import os \nos.environ.pop('TPU_PROCESS_ADDRESSES') \nos.environ.pop('CLOUD_TPU_TASK_ID')\nos.environ['XLA_USE_BF16']=\"1\"\nos.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'","metadata":{"execution":{"iopub.status.busy":"2024-06-21T09:39:32.673778Z","iopub.execute_input":"2024-06-21T09:39:32.674583Z","iopub.status.idle":"2024-06-21T09:39:32.679745Z","shell.execute_reply.started":"2024-06-21T09:39:32.674537Z","shell.execute_reply":"2024-06-21T09:39:32.678986Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from rdkit import Chem\nfrom rdkit.Chem import rdchem\nimport torch\nimport torchvision\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.distributed.parallel_loader as pl\nfrom torch import nn, Tensor\nfrom torchvision.transforms import v2\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport torch.utils.data as data\nimport pandas as pd\nimport time\nimport torch.nn as nn\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem\nimport numpy as np\nimport torch.nn.functional as F\nimport re\nimport time \nimport random\nimport duckdb","metadata":{"execution":{"iopub.status.busy":"2024-06-21T09:39:36.266551Z","iopub.execute_input":"2024-06-21T09:39:36.266937Z","iopub.status.idle":"2024-06-21T09:39:36.272831Z","shell.execute_reply.started":"2024-06-21T09:39:36.266905Z","shell.execute_reply":"2024-06-21T09:39:36.272159Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import gc;gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T09:34:26.535609Z","iopub.execute_input":"2024-06-21T09:34:26.536153Z","iopub.status.idle":"2024-06-21T09:34:26.764639Z","shell.execute_reply.started":"2024-06-21T09:34:26.536106Z","shell.execute_reply":"2024-06-21T09:34:26.763699Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"train_path = '/kaggle/input/leash-BELKA/train.parquet'\ntest_path = '/kaggle/input/leash-BELKA/test.parquet'\n\ncon = duckdb.connect()\n\ndf = con.query(f\"\"\"(SELECT *\n                        FROM parquet_scan('{train_path}')\n                        WHERE binds = 0\n                        ORDER BY random()\n                        LIMIT 2950000)\n                        UNION ALL\n                        (SELECT *\n                        FROM parquet_scan('{train_path}')\n                        WHERE binds = 1\n                        ORDER BY random()\n                        LIMIT 2950000)\"\"\").df()\n\ncon.close()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T09:34:27.724566Z","iopub.execute_input":"2024-06-21T09:34:27.725707Z","iopub.status.idle":"2024-06-21T09:38:54.488891Z","shell.execute_reply.started":"2024-06-21T09:34:27.725649Z","shell.execute_reply":"2024-06-21T09:38:54.488019Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"Chiral = {\"CHI_UNSPECIFIED\":50,  \"CHI_TETRAHEDRAL_CW\":51, \"CHI_TETRAHEDRAL_CCW\":52, \"CHI_OTHER\":53}\nHybridization = {\"UNSPECIFIED\":54, \"S\":55, \"SP\":56, \"SP2\":57, \"SP3\":58, \"SP3D\":59, \"SP3D2\":60, \"OTHER\":61}\nenc_atom = {\"H\": 0, \"C\": 1, \"O\": 2, \"N\": 3, \"S\": 4, \"F\": 5, \"D\": 6, \"B\": 7, \"I\": 8, \"Dy\":9, \"Br\":10, \"Cl\":11, \"Si\":12}\nenc_num_hs = {0:13, 1: 14, 2: 15, 3: 16, 4:17, 5:18, 6:19, 7:20, 8:21}\nenc_total_degree = {0: 22, 1: 23, 2: 24, 3: 25, 4:26}\nenc_Is_In_ring = {0: 27, 1: 28}\nenc_Is_Aromatic = {0: 29, 1: 30}\nenc_formal_charge = {0:31, 1: 32, 2: 33, 3: 34, 4:35, 5:36, 6:37, 7:38, 8:39, -1:40}\nenc_get_total_valence = {0:41, 1: 42, 2:43, 3: 44, 4:45, 5:46, 6:47, 7:48, 8:49}\nenc_not_smile = {\"(\": 62, \")\" : 63, \"[\": 64, \"]\":65, \"=\":66, \"#\":67, \"/\":68, \"@\":69, \"1\": 70, \"2\": 71, \"3\":72, \"4\":73, \"5\": 74, \"6\": 75, \"7\":76, '8':77, \"9\": 78, \"+\":79, \"-\":80}\nH_Vector = 0\nlowerReg = re.compile(r'^[a-z]+$')\ndef islower(s):\n    return lowerReg.match(s) is not None\n\nupperReg = re.compile(r'^[A-Z]+$')\ndef isupper(s):\n    return upperReg.match(s) is not None\n\ndef calc_atom_feature(atom):\n    feature = [enc_atom[atom.GetSymbol()]]\n    feature.append(enc_num_hs[atom.GetTotalNumHs()])\n    feature.append(enc_total_degree[atom.GetTotalDegree()])\n    feature.append(enc_Is_In_ring[atom.IsInRing()*1])\n    feature.append(enc_Is_Aromatic[atom.GetIsAromatic()*1])\n    feature.append(enc_formal_charge[atom.GetFormalCharge()])\n    feature.append(enc_get_total_valence[atom.GetTotalValence()])\n    feature.append(Chiral[str(atom.GetChiralTag())])\n    feature.append(Hybridization[str(atom.GetHybridization())])\n    return(feature)\n\n\ndef calc_structure_feature(c):\n    return enc_not_smile[c]\n#     elif c== '+' :\n#         feature[8] = 1\n#         flag = 1\n#     elif c== '-' :\n#         feature[9] = 1\n#         flag = 1\n#     elif c.isdigit() == True:\n#         if flag == 0:\n#             if c in label:\n#                 feature[20] = 1\n#             else:\n#                 label.append(c)\n#                 feature[19] = 1\n#         else:\n#             feature[int(c)-1+12] = 1\n#             flag = 0\n#     return(feature,flag,label)\ndef calc_featurevector(mol, smiles):\n    molfeature=[]\n    idx = 0\n    for c in smiles:\n        if islower(c) == True: \n                if c == \"y\":\n                    molfeature.append(81)\n                if c == \"r\":\n                    molfeature.append(82)\n                if c == \"s\":\n                    molfeature.append(83)\n                if c == \"c\":\n                    molfeature.append(84)\n                if c == \"0\":\n                    molfeature.append(85)\n                if c == \"n\":\n                    molfeature.append(86)\n                if c == \"i\":\n                    molfeature.append(87)\n        elif isupper(c) == True:\n            if c == 'H':\n                molfeature.append(H_Vector)\n            else:\n                molfeature.extend(calc_atom_feature(rdchem.Mol.GetAtomWithIdx(mol, idx)))\n                idx = idx + 1         \n        else:   \n            f = calc_structure_feature(c)\n            molfeature.append(f)\n    return(molfeature)\n\ndef mol_to_feature(mol,n):\n    try: defaultSMILES = Chem.MolToSmiles(mol, kekuleSmiles=False, isomericSmiles=True, rootedAtAtom=int(n))\n    except: defaultSMILES = Chem.MolToSmiles(mol, kekuleSmiles=False, isomericSmiles=True)\n    try: isomerSMILES = Chem.MolToSmiles(mol, kekuleSmiles=True, isomericSmiles=True, rootedAtAtom=int(n))\n    except: isomerSMILES = Chem.MolToSmiles(mol, kekuleSmiles=True, isomericSmiles=True)\n    return calc_featurevector(Chem.MolFromSmiles(defaultSMILES), isomerSMILES)\n\ndef mol_to_allSMILESfeature(mol, atomsize):\n    idx, features =0,  []\n    while idx < mol.GetNumAtoms():\n        try: defaultSMILES = Chem.MolToSmiles(mol, kekuleSmiles=False, isomericSmiles=True, rootedAtAtom=int(idx))\n        except: break\n        isomerSMILES = Chem.MolToSmiles(mol, kekuleSmiles=True, isomericSmiles=True, rootedAtAtom=int(idx))\n        features.append(calc_featurevector(Chem.MolFromSmiles(defaultSMILES), isomerSMILES,atomsize))\n        idx = idx + 1\n    return features","metadata":{"execution":{"iopub.status.busy":"2024-06-21T09:38:54.490511Z","iopub.execute_input":"2024-06-21T09:38:54.490801Z","iopub.status.idle":"2024-06-21T09:38:54.513005Z","shell.execute_reply.started":"2024-06-21T09:38:54.490773Z","shell.execute_reply":"2024-06-21T09:38:54.512237Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class Block(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.block = nn.Sequential(\n                        nn.Conv1d(dim, dim, kernel_size=1, groups=dim),\n                        nn.BatchNorm1d(dim, eps=1e-06),\n                        Permute([0, 2, 1]),\n                        nn.Linear(dim, dim*4),\n                        nn.GELU(),\n                        nn.Linear(dim*4, dim), Permute([0, 2, 1]), \n            nn.Conv1d(dim, dim, kernel_size=1, groups=dim),\n                        nn.BatchNorm1d(dim, eps=1e-06),\n                        Permute([0, 2, 1]),\n                        nn.Linear(dim, dim*4),\n                        nn.GELU(),\n                        nn.Linear(dim*4, dim), Permute([0, 2, 1]), \n            nn.Conv1d(dim, dim, kernel_size=1, groups=dim),\n                        nn.BatchNorm1d(dim, eps=1e-06),\n                        Permute([0, 2, 1]),\n                        nn.Linear(dim, dim*4),\n                        nn.GELU(),\n                        nn.Linear(dim*4, dim), Permute([0, 2, 1]), \n            nn.Conv1d(dim, dim, kernel_size=1, groups=dim),\n                        nn.BatchNorm1d(dim, eps=1e-06),\n                        Permute([0, 2, 1]),\n                        nn.Linear(dim, dim*4),\n                        nn.GELU(),\n                        nn.Linear(dim*4, dim), Permute([0, 2, 1]))\n    def forward(self, x):\n        out = self.block(x)\n        out += x\n        return out\nclass MyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        block = Block\n        layer = []\n        layer.append(nn.Sequential(nn.Conv1d(1, 2, kernel_size=351), nn.BatchNorm1d(2, eps=1e-06)))\n        dim=2\n        size = 350\n        for z in range(6):\n            if z != 2 and z != 3 and z != 4:\n                for i in range(3):\n                    layer.append(block(dim=dim))\n            else: \n                for i in range(12):\n                    layer.append(block(dim=dim))\n            size /= 2\n            layer.append(nn.Sequential(nn.BatchNorm1d(dim), nn.Conv1d(dim, dim*2, kernel_size=int(size)+ 1)))\n            dim *= 2\n        for i in range(3):\n            layer.append(block(dim=dim))\n        self.features = nn.Sequential(*layer)\n        self.classifier = nn.Sequential(nn.AdaptiveAvgPool1d(1), nn.BatchNorm1d(dim), nn.Flatten(1), nn.Linear(dim, 2))\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-21T09:38:54.513929Z","iopub.execute_input":"2024-06-21T09:38:54.514206Z","iopub.status.idle":"2024-06-21T09:38:54.528399Z","shell.execute_reply.started":"2024-06-21T09:38:54.514181Z","shell.execute_reply":"2024-06-21T09:38:54.527765Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Mydataset(Dataset):\n    def __init__(self, mol_value, binds, names, transform):\n        self.mol_value = mol_value\n        self.binds = binds\n        self.transform = transform\n        self.names = names\n    def __len__(self):\n        return len(self.mol_value)\n    def __getitem__ (self, idx):\n        mol_value = self.mol_value[idx]\n        bind = self.binds[idx]\n        names = self.names[idx]\n        if bind == 1:\n            bind = torch.tensor([[0, 1]]).type(torch.float32)\n        else:\n            bind = torch.tensor([[1, 0]]).type(torch.float32)\n        mol_value = mol_to_feature(Chem.MolFromSmiles(mol_value), -1)\n        length = len(mol_value)\n        if names == \"BRD4\":\n            mol_value.append(88)\n        if names == \"HSA\":\n            mol_value.append(89)\n        if names == \"sEH\":\n            mol_value.append(90)\n        mol_value.extend([-1]*(800-length))\n        mol_value = [((i+1)/91) for i in mol_value]\n        if self.transform:\n            mol_value = torch.from_numpy(np.array(mol_value))\n            mol_value = self.transform(mol_value)\n        return mol_value, bind\nclass Mytest(Dataset):\n    def __init__(self, mol_value, names, transform):\n        self.mol_value = mol_value\n        self.transform = transform\n        self.names = names\n    def __len__(self):\n        return len(self.mol_value)\n    def __getitem__ (self, idx):\n        mol_value = self.mol_value[idx]\n        names = self.names[idx]\n        mol_value = mol_to_feature(Chem.MolFromSmiles(mol_value), -1)\n        length = len(mol_value)\n        mol_value = mol_to_feature(Chem.MolFromSmiles(mol_value), -1)\n        length = len(mol_value)\n        if names == \"BRD4\":\n            mol_value.append(88)\n        if names == \"HSA\":\n            mol_value.append(89)\n        if names == \"sEH\":\n            mol_value.append(90)\n        mol_value.extend([-1]*(700-length))\n        mol_value = [((i+1)/91) for i in mol_value]\n        if self.transform:\n            mol_value = torch.from_numpy(np.array(mol_value))\n            mol_value = sel","metadata":{"execution":{"iopub.status.busy":"2024-06-21T09:43:29.336402Z","iopub.execute_input":"2024-06-21T09:43:29.336822Z","iopub.status.idle":"2024-06-21T09:43:29.346008Z","shell.execute_reply.started":"2024-06-21T09:43:29.336787Z","shell.execute_reply":"2024-06-21T09:43:29.345205Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def reduce_fn(vals):\n    # take average\n    return sum(vals) / len(vals)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T09:43:29.546296Z","iopub.execute_input":"2024-06-21T09:43:29.547078Z","iopub.status.idle":"2024-06-21T09:43:29.550538Z","shell.execute_reply.started":"2024-06-21T09:43:29.547041Z","shell.execute_reply":"2024-06-21T09:43:29.549757Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"def train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n    model.train() # put model in training mode\n    for bi, d in enumerate(data_loader): # enumerate through the dataloader\n\n        ids = d[0] # obtain the ids\n        targets = d[1] # obtain the targets\n\n        # put tensors onto desired device, in this case the TPU core\n        ids = ids.to(device, dtype=torch.long) \n        targets = targets.to(device, dtype=torch.float)\n\n        # pass ids to model\n        optimizer.zero_grad()\n        outputs = model(ids)\n        # calculate loss\n        loss =  criterion(outputs, targets)\n        # since the loss is on all 8 cores, reduce the loss values and print the average (as defined in reduce_fn)\n        loss_reduced = xm.mesh_reduce('loss_reduce',loss,reduce_fn)\n        xm.master_print(loss_reduced)\n        #master_print will only print once (not from all 8 cores)\n        # backpropagate\n        loss.backward()\n        \n        # Use PyTorch XLA optimizer stepping\n        xm.optimizer_step(optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T09:47:13.334930Z","iopub.execute_input":"2024-06-21T09:47:13.335315Z","iopub.status.idle":"2024-06-21T09:47:13.342125Z","shell.execute_reply.started":"2024-06-21T09:47:13.335281Z","shell.execute_reply":"2024-06-21T09:47:13.341373Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"def _run(df):\n    BATCH_SIZE = 295000 # batch size (cannot have too high due to memory constraints)\n    EPOCHS = 10 # number of epochs\n    # defining data samplers and loaders \n    dataset = Mydataset(df[\"molecule_smiles\"], df[\"binds\"], df[\"protein_name\"], transform=v2.Compose([v2.ToDtype(torch.float32)]))\n    t_dataset = Mytest(df[\"molecule_smiles\"],df[\"protein_name\"], transform=v2.Compose([v2.ToDtype(torch.float32)]))\n    train_sampler = data.distributed.DistributedSampler(\n          dataset,\n          num_replicas=xm.xrt_world_size(), # tell PyTorch how many devices (TPU cores) we are using for training\n          rank=xm.get_ordinal(), # tell PyTorch which device (core) we are on currently\n          shuffle=True)\n\n    train_data_loader = DataLoader(\n        dataset,\n        batch_size=BATCH_SIZE,\n        sampler=train_sampler,\n        drop_last=True,\n        num_workers=0)\n    test_sampler = data.distributed.DistributedSampler(\n          t_dataset,\n          num_replicas=xm.xrt_world_size(), # tell PyTorch how many devices (TPU cores) we are using for training\n          rank=xm.get_ordinal(), # tell PyTorch which device (core) we are on currently\n          shuffle=False)\n    test_loader = DataLoader(\n        t_dataset,\n        batch_size=BATCH_SIZE, shuffle=False, sampler=test_sampler,\n        drop_last=False,\n        num_workers=0)\n    model = MyModel()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n    criterion = torch.nn.BCEWithLogitsLoss()\n    device = xm.xla_device() # our device (single TPU core)\n    model = model.to(device) # put model onto the TPU core\n    xm.master_print('done loading model')\n    xm.master_print('training on train dataset')\n    # Let's start training on the train set!\n    for epoch in range(EPOCHS):\n        gc.collect() # I use a lot of gc.collect() statement to hopefully prevent OOM problems\n        # We use ParallelLoader (provided by PyTorch XLA) for TPU-core-specific dataloading:\n        para_loader = pl.ParallelLoader(train_data_loader, [device]) \n        xm.master_print('parallel loader created... training now')\n        gc.collect()\n        train_loop_fn(para_loader.per_device_loader(device), model, optimizer, device)\n        xm.save({\"last_epoch\": epoch + 1,\"model_state\": model.state_dict(), \"optimizer\": optimizer.state_dict()}, \"/kaggle/working/checkpoint.pth.tar\")\n    model.eval()\n    df = pd.read_csv(\"/kaggle/input/leash-BELKA/test.csv\")\n    df1 = pd.DataFrame({\"id\", \"binds\"})\n    pre_z = 0\n    para_loader = pl.ParallelLoader(test_loader, [device])\n    test_loader = para_loader.per_device_loader(device)\n    for i , a in enumerate(test_loader):\n        out = model(a.to(device))\n        _, out = torch.max(out, 1)\n        for z in range(10000):\n            df1.loc[pre_z] = {\"id\": df[\"id\"][pre_z], \"binds\": out[z].item()}\n            pre_z += 1\n        output = []\n        if (i + 100000) % 1000000 == 0:\n            print(i)\n    pd.DataFrame.to_csv(\"/kaggle/working/submission.csv\", df1)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T09:47:13.573110Z","iopub.execute_input":"2024-06-21T09:47:13.573622Z","iopub.status.idle":"2024-06-21T09:47:13.584847Z","shell.execute_reply.started":"2024-06-21T09:47:13.573587Z","shell.execute_reply":"2024-06-21T09:47:13.584020Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"def _mp_fn(rank, flags):\n    a = _run(df)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T09:47:13.834827Z","iopub.execute_input":"2024-06-21T09:47:13.835164Z","iopub.status.idle":"2024-06-21T09:47:13.838972Z","shell.execute_reply.started":"2024-06-21T09:47:13.835133Z","shell.execute_reply":"2024-06-21T09:47:13.838292Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"FLAGS={}\nstart_time = time.time()\nif __name__ == '__main__':\n    xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","metadata":{"execution":{"iopub.status.busy":"2024-06-21T09:47:28.930143Z","iopub.execute_input":"2024-06-21T09:47:28.930667Z","iopub.status.idle":"2024-06-21T09:47:29.715350Z","shell.execute_reply.started":"2024-06-21T09:47:28.930634Z","shell.execute_reply":"2024-06-21T09:47:29.714084Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"WARNING:root:Unsupported nprocs (8), ignoring...\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1718963249.311719  123181 pjrt_api.cc:100] GetPjrtApi was found for tpu at /usr/local/lib/python3.10/site-packages/torch_xla/lib/libtpu.so\nI0000 00:00:1718963249.311718  123182 pjrt_api.cc:100] GetPjrtApi was found for tpu at /usr/local/lib/python3.10/site-packages/torch_xla/lib/libtpu.so\nI0000 00:00:1718963249.311824  123181 pjrt_api.cc:79] PJRT_Api is set for device type tpu\nI0000 00:00:1718963249.311828  123182 pjrt_api.cc:79] PJRT_Api is set for device type tpu\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1718963249.311787  123183 pjrt_api.cc:100] GetPjrtApi was found for tpu at /usr/local/lib/python3.10/site-packages/torch_xla/lib/libtpu.so\nI0000 00:00:1718963249.311850  123182 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\nI0000 00:00:1718963249.311848  123181 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\nI0000 00:00:1718963249.311898  123183 pjrt_api.cc:79] PJRT_Api is set for device type tpu\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1718963249.311835  123180 pjrt_api.cc:100] GetPjrtApi was found for tpu at /usr/local/lib/python3.10/site-packages/torch_xla/lib/libtpu.so\nI0000 00:00:1718963249.311918  123183 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\nI0000 00:00:1718963249.311946  123180 pjrt_api.cc:79] PJRT_Api is set for device type tpu\nI0000 00:00:1718963249.311974  123180 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\nWARNING: Logging before InitGoogle() is written to STDERR\nWARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1718963249.312045  123181 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8477 in any of the 4 ports provided in `tpu_process_addresses`=\"localhost:8476,localhost:8477,localhost:8478,localhost:8479\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:479\nE0000 00:00:1718963249.312049  123182 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8478 in any of the 4 ports provided in `tpu_process_addresses`=\"localhost:8476,localhost:8477,localhost:8478,localhost:8479\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:479\nWARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1718963249.312083  123183 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8479 in any of the 4 ports provided in `tpu_process_addresses`=\"localhost:8476,localhost:8477,localhost:8478,localhost:8479\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:479\nE0621 09:47:29.350010089  123595 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {grpc_status:2, created_time:\"2024-06-21T09:47:29.34997943+00:00\"}\nE0621 09:47:29.350125493  123727 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2024-06-21T09:47:29.350098583+00:00\", grpc_status:2}\nE0621 09:47:29.350232493  123705 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2024-06-21T09:47:29.350214956+00:00\", grpc_status:2}\nE0621 09:47:29.350927240  123717 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2024-06-21T09:47:29.350910082+00:00\", grpc_status:2}\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)","\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/concurrent/futures/process.py\", line 246, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"/usr/local/lib/python3.10/concurrent/futures/process.py\", line 205, in _process_chunk\n    return [fn(*args) for args in chunk]\n  File \"/usr/local/lib/python3.10/concurrent/futures/process.py\", line 205, in <listcomp>\n    return [fn(*args) for args in chunk]\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/runtime.py\", line 95, in wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py\", line 59, in _run_thread_per_device\n    initializer_fn(local_rank, local_world_size)\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/runtime.py\", line 95, in wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py\", line 122, in initialize_multiprocess\n    devices = xm.get_xla_supported_devices()\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/core/xla_model.py\", line 99, in get_xla_supported_devices\n    devices = torch_xla._XLAC._xla_get_devices()\nRuntimeError: Bad StatusOr access: UNKNOWN: TPU initialization failed: open(/dev/accel0): Operation not permitted: Operation not permitted; Couldn't open device: /dev/accel0; Unable to create Node RegisterInterface for node 0, config: go/debugonly   device_path: \"/dev/accel0\" mode: KERNEL debug_data_directory: \"\" dump_anomalies_only: true crash_in_debug_dump: false allow_core_dump: true; could not create driver instance\n\"\"\"","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[51], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mxmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_mp_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFLAGS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfork\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/runtime.py:95\u001b[0m, in \u001b[0;36mrequires_pjrt.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_pjrt():\n\u001b[1;32m     92\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` not implemented for XRT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     93\u001b[0m       fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/distributed/xla_multiprocessing.py:38\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;129m@xr\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_pjrt\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspawn\u001b[39m(fn,\n\u001b[1;32m      8\u001b[0m           args\u001b[38;5;241m=\u001b[39m(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m           daemon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m           start_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Enables multi processing based replication.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    return None.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpjrt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py:211\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, nprocs, start_method, args)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nprocs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m   logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported nprocs (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m), ignoring...\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m nprocs)\n\u001b[0;32m--> 211\u001b[0m \u001b[43mrun_multiprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspawn_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_method\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/runtime.py:95\u001b[0m, in \u001b[0;36mrequires_pjrt.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_pjrt():\n\u001b[1;32m     92\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` not implemented for XRT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     93\u001b[0m       fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py:171\u001b[0m, in \u001b[0;36mrun_multiprocess\u001b[0;34m(fn, start_method, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m   mp_fn \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m    166\u001b[0m       _run_thread_per_device,\n\u001b[1;32m    167\u001b[0m       local_world_size\u001b[38;5;241m=\u001b[39mnum_processes,\n\u001b[1;32m    168\u001b[0m       fn\u001b[38;5;241m=\u001b[39mfunctools\u001b[38;5;241m.\u001b[39mpartial(fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n\u001b[1;32m    169\u001b[0m       initializer_fn\u001b[38;5;241m=\u001b[39minitialize_multiprocess)\n\u001b[1;32m    170\u001b[0m   process_results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mmap(mp_fn, \u001b[38;5;28mrange\u001b[39m(num_processes))\n\u001b[0;32m--> 171\u001b[0m   replica_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m      \u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m          \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprocess_results\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _merge_replica_results(replica_results)\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py:172\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    165\u001b[0m   mp_fn \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m    166\u001b[0m       _run_thread_per_device,\n\u001b[1;32m    167\u001b[0m       local_world_size\u001b[38;5;241m=\u001b[39mnum_processes,\n\u001b[1;32m    168\u001b[0m       fn\u001b[38;5;241m=\u001b[39mfunctools\u001b[38;5;241m.\u001b[39mpartial(fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n\u001b[1;32m    169\u001b[0m       initializer_fn\u001b[38;5;241m=\u001b[39minitialize_multiprocess)\n\u001b[1;32m    170\u001b[0m   process_results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mmap(mp_fn, \u001b[38;5;28mrange\u001b[39m(num_processes))\n\u001b[1;32m    171\u001b[0m   replica_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m--> 172\u001b[0m       itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m    173\u001b[0m           result\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m process_results))\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _merge_replica_results(replica_results)\n","File \u001b[0;32m/usr/local/lib/python3.10/concurrent/futures/process.py:575\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[1;32m    570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;124;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    576\u001b[0m         element\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m element:\n","File \u001b[0;32m/usr/local/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n","File \u001b[0;32m/usr/local/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n","File \u001b[0;32m/usr/local/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n","File \u001b[0;32m/usr/local/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: Bad StatusOr access: UNKNOWN: TPU initialization failed: open(/dev/accel0): Operation not permitted: Operation not permitted; Couldn't open device: /dev/accel0; Unable to create Node RegisterInterface for node 0, config: go/debugonly   device_path: \"/dev/accel0\" mode: KERNEL debug_data_directory: \"\" dump_anomalies_only: true crash_in_debug_dump: false allow_core_dump: true; could not create driver instance"],"ename":"RuntimeError","evalue":"Bad StatusOr access: UNKNOWN: TPU initialization failed: open(/dev/accel0): Operation not permitted: Operation not permitted; Couldn't open device: /dev/accel0; Unable to create Node RegisterInterface for node 0, config: go/debugonly   device_path: \"/dev/accel0\" mode: KERNEL debug_data_directory: \"\" dump_anomalies_only: true crash_in_debug_dump: false allow_core_dump: true; could not create driver instance","output_type":"error"}]}]}